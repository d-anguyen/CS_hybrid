{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS_hybrid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "thEX9SlWPRtL",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from include import *\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import pywt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "GPU = True\n",
        "if GPU == True:\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    dtype = torch.cuda.FloatTensor\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "    print(\"num GPUs\",torch.cuda.device_count())\n",
        "    device = 'cuda'\n",
        "    if torch.cuda.device_count()==0:\n",
        "        dtype = torch.FloatTensor\n",
        "        device = 'cpu'\n",
        "else:\n",
        "    dtype = torch.FloatTensor\n",
        "    device = 'cpu'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dU4G6SqSPRtQ"
      },
      "source": [
        "# 1. Load a test image from a dataset (now : CelebA 128x128)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "12vIJSitPRtQ",
        "colab": {}
      },
      "source": [
        "#dataset = 'mnist' # 'mnist' or 'celeba'\n",
        "dataset = 'celeba'\n",
        "path = './test_data/' + dataset + '/' \n",
        "img_name = dataset + '2' # 1-5 (for celeba), 1-6 (for mnist)\n",
        "img_path = path + img_name + \".jpg\"\n",
        "img_pil = Image.open(img_path)\n",
        "\n",
        "if dataset == 'celeba':\n",
        "    cx=89\n",
        "    cy=121\n",
        "    img_pil = img_pil.crop((cx-64, cy - 64, cx + 64, cy+64))\n",
        "\n",
        "img_np = pil_to_np(img_pil)\n",
        "print('Dimensions of input image:', img_np.shape)\n",
        "img_np = img_np / np.max(img_np)\n",
        "\n",
        "\n",
        "img_np_orig = 1*img_np\n",
        "\n",
        "'''if dataset == 'celeba':\n",
        "    plt.imshow(img_np.transpose(1,2,0))\n",
        "else:\n",
        "    plt.imshow(img_np[0,:,:])\n",
        "    plt.gray()'''\n",
        "\n",
        "img_var = np_to_var(img_np).type(dtype)\n",
        "d = img_np.shape[1]\n",
        "out_ch = img_np.shape[0]\n",
        "d_image = img_np.size\n",
        "\n",
        "# normalize the pixels to [-1,1]\n",
        "img_var = 2*img_var -1\n",
        "\n",
        "grid = torchvision.utils.make_grid(img_var.clamp(min=-1, max=1), scale_each=True, normalize=True)\n",
        "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())    \n",
        "plt.axis('off')\n",
        "\n",
        "save_to = './recovery/'\n",
        "save_path= save_to + 'Original'+'_'+img_name+'.png'\n",
        "plt.savefig(save_path, bbox_inches='tight', pad_inches = 0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKy9aeLNPRtV",
        "colab": {}
      },
      "source": [
        "f = 0.3 #compression rate\n",
        "print('Compression rate is ', f)\n",
        "m_image = int(f*d_image)\n",
        "print('Number of measurements is ',m_image, ' for signal of length ', d_image)\n",
        "\n",
        "# random Gaussian measurement process\n",
        "\n",
        "A = torch.randn(m_image, d_image).to(device)/np.sqrt(m_image)\n",
        "x = img_var.to(device).reshape(d_image)\n",
        "y = torch.matmul(A,x).to(device)\n",
        "\n",
        "#noise = 0.1 * torch.randn(m_image).to(device) \n",
        "#y = y+ noise\n",
        "\n",
        "#latentDim = model.config.noiseVectorDim\n",
        "print(A.shape, x.shape, y.shape)\n",
        "mse = torch.nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2E1I7pwbPRtX"
      },
      "source": [
        "# 2. Compressed sensing using generative models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lHsg8pp2PRtY"
      },
      "source": [
        "## 2.1. Load a pretrained generative model on the dataset (now: PGGAN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LyWUnPMvPRtY",
        "colab": {}
      },
      "source": [
        "use_gpu = True if torch.cuda.is_available() else False\n",
        "\n",
        "# trained on high-quality celebrity faces \"celebA\" dataset\n",
        "# this model outputs 512 x 512 pixel images\n",
        "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub',\n",
        "                       'PGAN', model_name='celeba',\n",
        "                       pretrained=True, useGPU=use_gpu)\n",
        "# this model outputs 256 x 256 pixel images\n",
        "# model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub',\n",
        "#                        'PGAN', model_name='celebAHQ-256',\n",
        "#                        pretrained=True, useGPU=use_gpu)\n",
        "G = model.netG\n",
        "\n",
        "#G.eval()\n",
        "latentDim = model.config.noiseVectorDim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vKCqtuFrPRtd"
      },
      "source": [
        "## 2.2. CS using the loaded GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mqew9mnwPRte",
        "colab": {}
      },
      "source": [
        "t0= time.time()\n",
        "\n",
        "z0, mse_wrt_loss = CSGM(G, latentDim, y, A, device, num_iter = 1600, lr=0.1, lr_decay_epoch=0, factor = 0.7)\n",
        "x0 = G(z0)\n",
        "\n",
        "grid = torchvision.utils.make_grid(x0.clamp(min=-1, max=1), scale_each=True, normalize=True)\n",
        "plt.axis('off')\n",
        "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())\n",
        "\n",
        "save_path= save_to +'PGGAN'+'_'+img_name+'.png'\n",
        "plt.savefig(save_path,bbox_inches='tight', pad_inches = 0) \n",
        "    \n",
        "t1= time.time()\n",
        "print('\\nTime elapsed:',t1-t0)\n",
        "\n",
        "error_wrt_truth = mse(x0, img_var).item()\n",
        "print('\\nl2-recovery error:', error_wrt_truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sog8Rnj9PRth",
        "colab": {}
      },
      "source": [
        "plt.xlabel('optimizer iteration')\n",
        "plt.ylabel('recovery error')\n",
        "plt.semilogy(mse_wrt_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "koW5yH6jPRtk"
      },
      "source": [
        "# 3. Compressed Sensing using Deep decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UFaaAdhjPRtk"
      },
      "source": [
        "## 3.1. Define the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-BQ1YoxPRtl",
        "colab": {}
      },
      "source": [
        "#use decoder architecture or DC GAN architecture\n",
        "decodetype = 'upsample' # transposeconv / upsample\n",
        "\n",
        "num_channels = [10,20,40,60,80] \n",
        "\n",
        "output_depth = img_np.shape[0] # number of output channels\n",
        "net = autoencodernet(num_output_channels=output_depth,num_channels_up=num_channels,need_sigmoid=True, \n",
        "                        decodetype=decodetype\n",
        "                        ).type(dtype)\n",
        "\n",
        "print(\"number of parameters: \", num_param(net))\n",
        "if decodetype == 'upsample':\n",
        "    print(net.decoder)\n",
        "elif decodetype == 'transposeconv':\n",
        "    print(net.convdecoder)\n",
        "net_in = copy.deepcopy(net)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "juKFr7rSPRtn"
      },
      "source": [
        "## 3.2. CS using untrained network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R7q6GykdPRtn",
        "colab": {}
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "net, net_input, loss = CS_DD(net, num_channels, d_image, y=y, A=A, device= device, \n",
        "                             num_iter =8000, lr = 0.0002, lr_decay_epoch=3000, factor = 0.7, decodetype = decodetype)\n",
        "x_DD = 2*net( net_input.type(dtype) )-1 #.data.cpu().numpy()[0]\n",
        "\n",
        "t1 = time.time()\n",
        "grid = torchvision.utils.make_grid(x_DD.clamp(min=-1, max=1), scale_each=True, normalize=True)\n",
        "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())\n",
        "plt.axis('off')\n",
        "#plt.imshow(x_hat.transpose(1,2,0))\n",
        "#plt.show()\n",
        "\n",
        "print('\\n time elapsed:', t1-t0)\n",
        "\n",
        "error_wrt_truth = mse(x_DD, img_var).item()\n",
        "print('\\nl2-recovery error:', error_wrt_truth)\n",
        "\n",
        "save_path= save_to +'DD'+'_'+img_name+'.png'\n",
        "plt.savefig(save_path,bbox_inches='tight', pad_inches = 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdKpWoPOxcXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.xlabel('optimizer iteration')\n",
        "plt.ylabel('recovery error')\n",
        "plt.semilogy(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oqvdI7lbPRtp"
      },
      "source": [
        "# 4. Compressed sensing using hybrid model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4o3jF8xQPRtq"
      },
      "source": [
        "## 4.1. Define the untrained network used for hybrid model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xJn08Iz5PRtq",
        "colab": {}
      },
      "source": [
        "#use decoder architecture or DC GAN architecture\n",
        "decodetype = 'upsample' # transposeconv / upsample\n",
        "\n",
        "num_channels = [40,40,40,40,40,40] \n",
        "\n",
        "output_depth = img_np.shape[0] # number of output channels\n",
        "net = autoencodernet(num_output_channels=output_depth,num_channels_up=num_channels,need_sigmoid=True, \n",
        "                        decodetype=decodetype\n",
        "                        ).type(dtype)\n",
        "\n",
        "print(\"number of parameters: \", num_param(net))\n",
        "#if decodetype == 'upsample':\n",
        "#    print(net.decoder)\n",
        "#elif decodetype == 'transposeconv':\n",
        "#    print(net.convdecoder)\n",
        "#net_copy = copy.deepcopy(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vR_To4ghPRts"
      },
      "source": [
        "## 4.2. CS using hybrid model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_3Mr2wxPPRtt",
        "colab": {}
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "#initialization by csgm\n",
        "print('Initialization step')\n",
        "z0, loss1 = CSGM(G=G, latentDim=latentDim, y=y, A=A, device=device, num_iter=500, lr = 0.1, lr_decay_epoch = 0)\n",
        "x0 = G(z0)\n",
        "print('error = ', mse(x0, img_var))\n",
        "\n",
        "net, net_input, loss2 = CS_DD(net, num_channels, d_image, y=y, A=A, device= device, \n",
        "                             num_iter = 500, lr = 0.0002, lr_decay_epoch=0, decodetype = decodetype)\n",
        "x_DD = 2*net( net_input.type(dtype) )-1 #.data.cpu().numpy()[0]\n",
        "print('error = ',  mse(x_DD, img_var).item())\n",
        "\n",
        "# performing optimization\n",
        "print('Performing optimization')\n",
        "net, net_input, z, alpha, beta, loss3 = CS_hybrid2(G, net, net_input, num_channels, d_image, y, A, device, \n",
        "              z0, latentDim, num_iter = 6000, lr_decay_epoch = 2000, factor = 0.7,\n",
        "              decodetype = decodetype, lr_alpha = 0.01, lr_beta = 0.01, lr_z = 0.05, lr_net = 0.0001, \n",
        "              alpha_init = 0.5, beta_init = 0.5)\n",
        "x_hat = alpha.clamp(0,1)*G(z) + beta.clamp(0,1)*(2*net(net_input.type(dtype)) - 1)\n",
        "print('error = ', mse(x_hat, img_var).item())\n",
        "\n",
        "print(alpha,beta)\n",
        "\n",
        "grid = torchvision.utils.make_grid(x_hat.clamp(min=-1, max=1), scale_each=True, normalize=True)\n",
        "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())\n",
        "plt.axis('off')\n",
        "\n",
        "t1 = time.time()\n",
        "print('\\n time elapsed:', t1-t0)\n",
        "\n",
        "error_wrt_truth = mse(x_hat, img_var).item()\n",
        "print('\\nl2-recovery error:', error_wrt_truth)\n",
        "\n",
        "\n",
        "save_path= save_to +'Hybrid'+'_'+img_name+'.png'\n",
        "plt.savefig(save_path,bbox_inches='tight', pad_inches = 0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47XYpz_yNaPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.xlabel('optimizer iteration')\n",
        "plt.ylabel('recovery error')\n",
        "loss = np.concatenate((loss1,loss2,loss3), axis = 0)\n",
        "plt.semilogy(loss3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NoIeVAhWmQW",
        "colab_type": "text"
      },
      "source": [
        "##4.3. Using default Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TS8xsAcjWtCL",
        "colab": {}
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "#initialization by csgm\n",
        "print('Initialization step')\n",
        "z0, loss1 = CSGM(G=G, latentDim=latentDim, y=y, A=A, device=device, num_iter=500, lr = 0.2, lr_decay_epoch = 0)\n",
        "x0 = G(z0)\n",
        "print('error = ', mse(x0, img_var))\n",
        "\n",
        "net, net_input, loss2 = CS_DD(net, num_channels, d_image, y=y, A=A, device= device, \n",
        "                             num_iter = 500, lr = 0.0002, lr_decay_epoch=0)\n",
        "x_DD = 2*net( net_input.type(dtype) )-1 #.data.cpu().numpy()[0]\n",
        "print('error = ',  mse(x_DD, img_var).item())\n",
        "\n",
        "# performing optimization\n",
        "print('Performing optimization')\n",
        "net, net_input, z, alpha, beta, loss3 = CS_hybrid_Adam(G, net, net_input, num_channels, d_image, y, A, device, \n",
        "              z0, latentDim, num_iter = 5000, decodetype = 'upsample')\n",
        "x_hat = alpha.clamp(0,1)*G(z) + beta.clamp(0,1)*(2*net(net_input.type(dtype)) - 1)\n",
        "print('error = ', mse(x_hat, img_var).item())\n",
        "\n",
        "print(alpha,beta)\n",
        "\n",
        "grid = torchvision.utils.make_grid(x_hat.clamp(min=-1, max=1), scale_each=True, normalize=True)\n",
        "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())\n",
        "plt.axis('off')\n",
        "\n",
        "t1 = time.time()\n",
        "print('\\n time elapsed:', t1-t0)\n",
        "\n",
        "error_wrt_truth = mse(x_hat, img_var).item()\n",
        "print('\\nl2-recovery error:', error_wrt_truth)\n",
        "\n",
        "\n",
        "save_path= save_to +'Hybrid'+'_'+img_name+'.png'\n",
        "plt.savefig(save_path,bbox_inches='tight', pad_inches = 0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EiftS4jxi8Bk",
        "colab": {}
      },
      "source": [
        "plt.xlabel('optimizer iteration')\n",
        "plt.ylabel('recovery error')\n",
        "loss = np.concatenate((loss1,loss2,loss3), axis = 0)\n",
        "plt.semilogy(loss3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz7sZMm_EEeM",
        "colab_type": "text"
      },
      "source": [
        "#5. Image adaptive GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4D0ZW9LuDlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G_IA = copy.deepcopy(G)\n",
        "\n",
        "t0= time.time()\n",
        "\n",
        "z0, mse_wrt_loss = CSGM(G, latentDim, y, A, device, num_iter = 500, lr=0.2, lr_decay_epoch=0, factor = 0.7)\n",
        "x0 = G(z0)\n",
        "\n",
        "z_IA, mse_wrt_loss = IA(G_IA, latentDim, y, A, z0, device, num_iter= 5000)\n",
        "x_IA = G_IA(z_IA)\n",
        "\n",
        "grid = torchvision.utils.make_grid(x_IA.clamp(min=-1, max=1), scale_each=True, normalize=True)\n",
        "plt.axis('off')\n",
        "plt.imshow(grid.detach().permute(1, 2, 0).cpu().numpy())\n",
        "\n",
        "save_path= save_to +'IAGAN'+'_'+img_name+'.png'\n",
        "plt.savefig(save_path,bbox_inches='tight', pad_inches = 0) \n",
        "    \n",
        "t1= time.time()\n",
        "print('\\nTime elapsed:',t1-t0)\n",
        "\n",
        "error_wrt_truth = mse(x_IA, img_var).item()\n",
        "print('\\nl2-recovery error:', error_wrt_truth)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}